{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f53b588f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 15:12:30,785 - INFO - Starting document loading from: D:\\Muskan.Verma_OneDrive_Data\\OneDrive - Course5 Intelligence Limited\\Desktop\\ai\\dataset\n",
      "2025-07-24 15:12:30,788 - INFO - Files found:\n",
      "2025-07-24 15:12:30,790 - INFO -   .pdf: 3 files\n",
      "2025-07-24 15:12:30,791 - INFO -   .docx: 1 files\n",
      "2025-07-24 15:12:30,823 - INFO - Loaded PDF: Case study-FCFF with basics.pdf (1 pages)\n",
      "2025-07-24 15:12:30,881 - INFO - Loaded Word document: Case_Valuation in LBO transaction.docx\n",
      "2025-07-24 15:12:30,951 - INFO - Loaded PDF: Lecture Intro on APV.pdf (15 pages)\n",
      "2025-07-24 15:12:31,466 - INFO - Loaded PDF: Optional article_Using APV_ A Better Tool for Valuing Operations.pdf (15 pages)\n",
      "2025-07-24 15:12:31,467 - INFO - Total documents loaded: 32\n",
      "2025-07-24 15:12:31,469 - INFO - Created 54 chunks from 32 documents\n",
      "2025-07-24 15:12:31,477 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "2025-07-24 15:12:34,643 - INFO - Creating FAISS vector store...\n",
      "2025-07-24 15:12:35,814 - INFO - Vector store saved to: multi_format_faiss_index\n",
      "2025-07-24 15:12:35,816 - INFO - Document processing completed successfully!\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, UnstructuredFileLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import UnstructuredWordDocumentLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    " \n",
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    " \n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    " \n",
    " \n",
    "class MultiDocumentLoader:\n",
    "    def __init__(self, folder_path: str):\n",
    "        self.folder_path = Path(folder_path)\n",
    "        self.supported_extensions = {\n",
    "            '.pdf': self._load_pdf,\n",
    "            '.doc': self._load_word,\n",
    "            '.docx': self._load_word,\n",
    "        }\n",
    " \n",
    "    def _validate_folder(self) -> bool:\n",
    "        if not self.folder_path.exists():\n",
    "            raise FileNotFoundError(f\"Directory {self.folder_path} does not exist\")\n",
    "        if not self.folder_path.is_dir():\n",
    "            raise ValueError(f\"{self.folder_path} is not a directory\")\n",
    "        return True\n",
    " \n",
    "    def _get_files_by_type(self) -> Dict[str, List[Path]]:\n",
    "        files_by_type = {}\n",
    "        for file_path in self.folder_path.rglob('*'):\n",
    "            if file_path.is_file():\n",
    "                extension = file_path.suffix.lower()\n",
    "                if extension not in files_by_type:\n",
    "                    files_by_type[extension] = []\n",
    "                files_by_type[extension].append(file_path)\n",
    "        return files_by_type\n",
    " \n",
    "    def _load_pdf(self, file_path: Path) -> List[Document]:\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(file_path))\n",
    "            documents = loader.load()\n",
    "            logger.info(f\"Loaded PDF: {file_path.name} ({len(documents)} pages)\")\n",
    "            return documents\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading PDF {file_path.name}: {e}\")\n",
    "            return []\n",
    " \n",
    "    def _load_word(self, file_path: Path) -> List[Document]:\n",
    "        try:\n",
    "            loader = UnstructuredWordDocumentLoader(str(file_path))\n",
    "            documents = loader.load()\n",
    "            logger.info(f\"Loaded Word document: {file_path.name}\")\n",
    "            return documents\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading Word document {file_path.name}: {e}\")\n",
    "            return []\n",
    " \n",
    "    def _load_other_files(self, file_path: Path) -> List[Document]:\n",
    "        try:\n",
    "            loader = UnstructuredFileLoader(str(file_path))\n",
    "            documents = loader.load()\n",
    "            logger.info(f\"Loaded file: {file_path.name}\")\n",
    "            return documents\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading file {file_path.name}: {e}\")\n",
    "            return []\n",
    " \n",
    "    def load_documents(self) -> List[Document]:\n",
    "        self._validate_folder()\n",
    "        logger.info(f\"Starting document loading from: {self.folder_path}\")\n",
    "        files_by_type = self._get_files_by_type()\n",
    "        logger.info(\"Files found:\")\n",
    "        for ext, files in files_by_type.items():\n",
    "            logger.info(f\"  {ext}: {len(files)} files\")\n",
    "        all_documents = []\n",
    "        for file_path in self.folder_path.rglob('*'):\n",
    "            if file_path.is_file():\n",
    "                extension = file_path.suffix.lower()\n",
    "                if extension in self.supported_extensions:\n",
    "                    documents = self.supported_extensions[extension](file_path)\n",
    "                else:\n",
    "                    documents = self._load_other_files(file_path)\n",
    "                all_documents.extend(documents)\n",
    "        logger.info(f\"Total documents loaded: {len(all_documents)}\")\n",
    "        return all_documents\n",
    " \n",
    " \n",
    "def create_vector_store(documents: List[Document],\n",
    "                        model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "                        chunk_size: int = 1000,\n",
    "                        chunk_overlap: int = 150,\n",
    "                        save_path: str = \"multi_format_faiss_index\") -> FAISS:\n",
    "    if not documents:\n",
    "        raise ValueError(\"No documents provided for vector store creation\")\n",
    " \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    chunked_docs = text_splitter.split_documents(documents)\n",
    "    logger.info(f\"Created {len(chunked_docs)} chunks from {len(documents)} documents\")\n",
    " \n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=model_name,\n",
    "        model_kwargs={'device': 'cpu'},\n",
    "        encode_kwargs={'normalize_embeddings': False}\n",
    "    )\n",
    " \n",
    "    logger.info(\"Creating FAISS vector store...\")\n",
    "    vectorstore = FAISS.from_documents(chunked_docs, embeddings)\n",
    "    vectorstore.save_local(save_path)\n",
    "    logger.info(f\"Vector store saved to: {save_path}\")\n",
    "    return vectorstore\n",
    " \n",
    " \n",
    "def main():\n",
    "    try:\n",
    "        folder_path = r\"D:\\Muskan.Verma_OneDrive_Data\\OneDrive - Course5 Intelligence Limited\\Desktop\\ai\\dataset\"\n",
    "        doc_loader = MultiDocumentLoader(folder_path)\n",
    "        documents = doc_loader.load_documents()\n",
    " \n",
    "        if not documents:\n",
    "            logger.warning(\"No documents were loaded.\")\n",
    "            return\n",
    " \n",
    "        create_vector_store(\n",
    "            documents=documents,\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=150,\n",
    "            save_path=\"multi_format_faiss_index\"\n",
    "        )\n",
    " \n",
    "        logger.info(\"Document processing completed successfully!\")\n",
    " \n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    " \n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5cfbc2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rag_ai_env)",
   "language": "python",
   "name": "rag_ai_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
